The rapid growth in data makes the quest for highly scalable learners.  To achieve the  trade-off between structure complexity and classification accuracy, the k-dependence Bayesian classifier (KDB)  allows to represent different number of interdependencies for different data sizes. In this paper we investigate two extensions to KDB, minimal-redundancy-maximal-relevance (mRMR) analysis and discriminative model selection (DMS). The mRMR analysis sorts the predictive features to identify redundant ones, DMS selects an optimal sub-model by removing redundant features or relevant interdependencies. Experimental results on 40 UCI datasets demonstrate that these two techniques are complementary and the proposed algorithm achieves competitive classification performance, and less classification time than other state-of-the-art Bayesian network classifiers like TAN and AODE.
