# MMKDB
The rapid growth in data makes ever more urgent the quest for highly scalable learners that have better classification performance and simpler structures. Discarding redundant features can both reduce classification accuracy and classification time. The $K$-dependence Bayesian classifier (KDB) is one of the famous Bayesian network classifiers (BNCs). It allows the number of interdependencies to be adjusted to best accommodate the best trade-offs between structure complexity and classification accuracy for different data quantities. To identify the features with redundant information and remove them when necessary, in this paper we investigate two extensions to KDB, minimal-redundancy-maximal-relevance analysis and discriminative model selection. Furthermore, we demonstrate that the techniques are complementary. Experimental results on 40 UCI datasets demonstrate that the proposed algorithm achieves competitive classification performance, and less classification time than other state-of-the-art BNCs, e.g., NB, TAN, KDB and AODE.
